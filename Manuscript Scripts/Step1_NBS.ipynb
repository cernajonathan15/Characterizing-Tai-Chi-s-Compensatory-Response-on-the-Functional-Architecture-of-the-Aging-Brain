{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "6953296e-e780-40ed-8674-374e6a2f2136",
      "cell_type": "code",
      "source": "# =============================================================================\n# 1. Imports and Global Settings\n# =============================================================================\nimport warnings\nimport logging\nimport datetime\nimport time\nimport pickle\nimport numpy as np\nimport os\nimport re\nimport sys\nimport random\nimport itertools\nimport pandas as pd\nfrom scipy import stats\nimport networkx as nx\nfrom scipy.sparse.csgraph import connected_components\nfrom scipy.sparse import csr_matrix\nfrom dataclasses import dataclass\nimport concurrent.futures\nfrom numba import jit, prange\nfrom typing import Dict, List, Tuple, Optional\nfrom functools import partial\nfrom sklearn.model_selection import KFold\nfrom IPython.display import clear_output\n\n# =============================================================================\n# 2. Data Classes\n# =============================================================================\n@dataclass\nclass ConvergenceStats:\n    \"\"\"Track convergence statistics for NBS permutations.\"\"\"\n    window_size: int = 50\n    alpha: float = 0.05\n    min_iterations: int = 100\n\n    def __post_init__(self):\n        self.component_sizes = []\n        self.running_mean = []\n        self.running_var = []\n        self.converged = False\n\n    def update(self, size: float) -> bool:\n        \"\"\"Update convergence statistics with the new component size.\"\"\"\n        self.component_sizes.append(size)\n        if len(self.component_sizes) >= self.min_iterations:\n            recent_sizes = self.component_sizes[-self.window_size:]\n            mean = np.mean(recent_sizes)\n            var = np.var(recent_sizes)\n            self.running_mean.append(mean)\n            self.running_var.append(var)\n            if var < self.alpha:\n                self.converged = True\n                return True\n        return False\n\n# =============================================================================\n# 3. Helper Functions (Statistical, Parsing, and Edge Utilities)\n# =============================================================================\ndef calculate_effect_sizes(component: set, data1: np.ndarray, data2: np.ndarray) -> dict:\n    \"\"\"\n    Calculate multiple effect size metrics for NBS components.\n    \n    Args:\n        component: Set of nodes in the component.\n        data1, data2: Original connectivity data for both groups.\n    \n    Returns:\n        Dictionary of effect size metrics.\n    \"\"\"\n    effect_sizes = {}\n    \n    for node in component:\n        # Calculate Cohen's d\n        d = (np.mean(data1[:, node]) - np.mean(data2[:, node])) / \\\n            np.sqrt((np.var(data1[:, node]) + np.var(data2[:, node])) / 2)\n            \n        # Calculate rank-biserial correlation\n        u_stat, _ = stats.mannwhitneyu(data1[:, node], data2[:, node])\n        n1, n2 = len(data1), len(data2)\n        rank_biserial = 2 * (u_stat / (n1 * n2)) - 1\n        \n        effect_sizes[node] = {\n            'cohens_d': d,\n            'rank_biserial': rank_biserial\n        }\n    \n    return effect_sizes\n\n\ndef validate_nbs_results(results: dict, data1: np.ndarray, data2: np.ndarray, alpha: float = 0.05) -> dict:\n    \"\"\"\n    Validate NBS results using additional statistical tests and account for dynamic nature of data.\n    \n    Args:\n        results: NBS analysis results.\n        data1, data2: Original connectivity data.\n        alpha: Significance level.\n    \n    Returns:\n        Dictionary with validation metrics including dynamic connection counts.\n    \"\"\"\n    validation = {}\n    \n    for idx, component in enumerate(results['significant_components']):\n        # Cross-validation\n        cv_scores = []\n        kf = KFold(n_splits=5, shuffle=True)\n        \n        for train_idx, test_idx in kf.split(data1):\n            # Train/test split\n            train1, test1 = data1[train_idx], data1[test_idx]\n            train2, test2 = data2[train_idx], data2[test_idx]\n            \n            # Calculate component stability\n            component_pvals = []\n            for node in component:\n                _, p = stats.mannwhitneyu(\n                    test1[:, node],\n                    test2[:, node],\n                    alternative='two-sided'\n                )\n                component_pvals.append(p < alpha)\n            \n            # Calculate validation score\n            cv_scores.append(np.mean(component_pvals))\n        \n        validation[idx] = {\n            'cross_validation_score': np.mean(cv_scores),\n            'stability': np.std(cv_scores),\n            'dynamic_size': len(component) * len(np.unique(results.get('time_windows', [1])))\n        }\n    \n    return validation\n\n\ndef identify_compensatory_connections(\n    aging_results: dict, \n    taichi_results: dict,\n    yac_means: dict,  # For consistency, parameter name remains yac_means\n    comparison: str,\n    labels: np.ndarray,\n    edge_indices_reverse: dict,\n    group1_means_all: dict,\n    group2_means_all: dict\n) -> dict:\n    \"\"\"\n    Identify compensatory and deterioration connections specifically for TCOA vs OAC comparison.\n    YAC means are used as reference points.\n    Only processes mechanism categories for TCOA comparison, not for OAC vs YAC.\n    \"\"\"\n    results = {\n        'compensatory_components': [],\n        'effect_sizes': {},\n        'mechanisms': {}\n    }\n\n    if comparison != 'oac_vs_tcoa':\n        return results\n\n    for key in aging_results.keys():\n        aging_nbs_result = aging_results[key]\n        taichi_nbs_result = taichi_results.get(key)\n        if not taichi_nbs_result:\n            continue\n\n        yac_mean = yac_means.get(key)\n        if not yac_mean:\n            continue\n\n        for aging_comp_idx, aging_component in enumerate(aging_nbs_result['significant_components']):\n            aging_effect = aging_nbs_result['effect_sizes'][aging_comp_idx]['mean_effect']\n\n            for tc_comp_idx, tc_component in enumerate(taichi_nbs_result['significant_components']):\n                tc_effect = taichi_nbs_result['effect_sizes'][tc_comp_idx]['mean_effect']\n                common_edges = aging_component.intersection(tc_component)\n\n                if common_edges:\n                    for edge in common_edges:\n                        edge_nodes = aging_nbs_result['component_edges'][aging_comp_idx]\n                        for node_pair in edge_nodes:\n                            node1, node2 = node_pair\n                            network1 = get_network_name(labels[node1])\n                            network2 = get_network_name(labels[node2])\n                            if not network1 or not network2:\n                                continue\n\n                            node_networks = sorted([network1, network2])\n\n                            edge_idx = edge_indices_reverse.get((node1, node2))\n                            if edge_idx is None:\n                                edge_idx = edge_indices_reverse.get((node2, node1))\n                            if edge_idx is None:\n                                continue\n\n                            oac_mean = group1_means_all[key][edge_idx]\n                            tcoa_mean = group2_means_all[key][edge_idx]\n                            yac_mean_edge = yac_mean[edge_idx]\n\n                            oac_distance = abs(oac_mean - yac_mean_edge)\n                            tcoa_distance = abs(tcoa_mean - yac_mean_edge)\n\n                            mechanism = determine_mechanism(\n                                oac_distance, tcoa_distance, \n                                oac_mean, tcoa_mean, yac_mean_edge\n                            )\n\n                            results['compensatory_components'].append({\n                                'edge': edge_idx,\n                                'node1': node1,\n                                'node2': node2,\n                                'aging_component': aging_comp_idx,\n                                'mechanism': mechanism,\n                                'key': key,\n                                'network_pair': '-'.join(node_networks)\n                            })\n\n                            results['effect_sizes'][edge_idx] = {\n                                'aging': oac_mean,\n                                'intervention': tcoa_mean\n                            }\n                            results['mechanisms'][edge_idx] = mechanism\n\n    return results\n\n\ndef determine_mechanism(\n    oac_distance: float, \n    tcoa_distance: float, \n    oac_mean: float,\n    tcoa_mean: float,\n    yac_mean: float\n) -> str:\n    \"\"\"\n    Determine the mechanism based on distances and means.\n    \n    Args:\n        oac_distance: Distance between OAC and YAC means.\n        tcoa_distance: Distance between TCOA and YAC means.\n        oac_mean: Mean connectivity value for OAC group.\n        tcoa_mean: Mean connectivity value for TCOA group.\n        yac_mean: Mean connectivity value for YAC group.\n    \n    Returns:\n        str: The identified mechanism type.\n    \"\"\"\n    if tcoa_distance < oac_distance:\n        if tcoa_mean > yac_mean > oac_mean:\n            return 'Enhancement'\n        elif tcoa_mean < yac_mean < oac_mean:\n            return 'Normalization'\n        else:\n            same_side = ((tcoa_mean - yac_mean) * (oac_mean - yac_mean) > 0)\n            if same_side:\n                return 'Restoration'\n            else:\n                return 'Alternative'\n    else:\n        if tcoa_mean > yac_mean > oac_mean:\n            return 'Exacerbation'\n        elif tcoa_mean < yac_mean < oac_mean:\n            return 'Decompensation'\n        else:\n            same_side = ((tcoa_mean - yac_mean) * (oac_mean - yac_mean) > 0)\n            if same_side:\n                return 'Deterioration'\n            else:\n                return 'Maladaptive'\n\n\ndef get_edge_indices(n_nodes):\n    \"\"\"Get a mapping from edge indices to node pairs.\"\"\"\n    edge_indices = {}\n    idx = 0\n    for i in range(n_nodes):\n        for j in range(i + 1, n_nodes):\n            edge_indices[idx] = (i, j)\n            idx += 1\n    return edge_indices\n\n\ndef get_edge_indices_reverse(n_nodes):\n    \"\"\"Get a mapping from node pairs to edge indices.\"\"\"\n    edge_indices = {}\n    idx = 0\n    for i in range(n_nodes):\n        for j in range(i + 1, n_nodes):\n            edge_indices[(i, j)] = idx\n            edge_indices[(j, i)] = idx  # Include reverse mapping\n            idx += 1\n    return edge_indices\n\n\ndef get_network_name(label_name):\n    \"\"\"Extract the network name from the label name.\"\"\"\n    if 'Vis' in label_name:\n        return 'Visual'\n    elif 'SomMot' in label_name:\n        return 'Somatomotor'\n    elif 'DorsAttn' in label_name:\n        return 'DorsalAttention'\n    elif 'SalVentAttn' in label_name or 'VentAttn' in label_name:\n        return 'VentralAttention'\n    elif 'Limbic' in label_name:\n        return 'Limbic'\n    elif 'Cont' in label_name:\n        return 'Frontoparietal'   \n    elif 'Default' in label_name:\n        return 'Default'\n    else:\n        return None\n\n\ndef get_region_name(label_name):\n    \"\"\"Extract the region name from the label name.\"\"\"\n    parts = label_name.split('-')\n    network_name = get_network_name(label_name)\n    if network_name == 'Visual' or network_name == 'Somatomotor':\n        region_info = parts[0].split('_')[2:]\n    else:\n        region_info = parts[0].split('_')[3:]\n    hemisphere = parts[1]\n    return '_'.join(region_info) + '-' + hemisphere\n\n\ndef parse_within_conn_value(value):\n    \"\"\"Parse within-network connectivity string.\"\"\"\n    pattern = r'\\[(.*?)\\]: (.*?) - (.*?) - (.*)'\n    match = re.match(pattern, value)\n    if match:\n        network = match.group(1)\n        region1 = match.group(2)\n        connectivity = float(match.group(3))\n        region2 = match.group(4)\n        return network, region1, connectivity, region2\n    else:\n        raise ValueError(f\"Unable to parse within network connectivity value: {value}\")\n\n\ndef parse_between_conn_value(value):\n    \"\"\"Parse between-network connectivity string.\"\"\"\n    pattern = r'\\[(.*?), (.*?)\\]: (.*?) - (.*?) - (.*)'\n    match = re.match(pattern, value)\n    if match:\n        network1, network2 = sorted([match.group(1), match.group(2)])\n        region1 = match.group(3)\n        connectivity = float(match.group(4))\n        region2 = match.group(5)\n        return [network1, network2], region1, connectivity, region2\n    else:\n        raise ValueError(f\"Unable to parse between network connectivity value: {value}\")\n\n\ndef parse_connectivity_data(connectivity_data, connectivity_type, participants):\n    \"\"\"Parse the within or between network connectivity data per participant.\"\"\"\n    print(\"\\nParsing connectivity data...\")\n    parsed_data = {}\n    total_participants = len(set(key[0] for key in connectivity_data.keys()))\n    participants_processed = 0\n\n    # Group keys by participant\n    participant_keys = {}\n    for key in connectivity_data.keys():\n        participant = key[0]\n        if participant not in participant_keys:\n            participant_keys[participant] = []\n        participant_keys[participant].append(key)\n\n    for participant in participants:\n        if participant not in participant_keys:\n            continue  # Skip participants without data\n        for conn_type in ['within', 'between']:\n            parsed_data_participant = {}\n            for key in participant_keys[participant]:\n                if connectivity_type == conn_type:\n                    values = connectivity_data[key]\n                    parsed_values = []\n                    for value in values:\n                        if conn_type == 'within':\n                            network, region1, conn_value, region2 = parse_within_conn_value(value)\n                            parsed_values.append((network, region1, conn_value, region2))\n                        elif conn_type == 'between':\n                            networks_pair, region1, conn_value, region2 = parse_between_conn_value(value)\n                            parsed_values.append((networks_pair, region1, conn_value, region2))\n                    parsed_data_participant[key] = parsed_values\n            parsed_data.update(parsed_data_participant)\n        participants_processed += 1\n        sys.stdout.write(f\"\\rProcessed participant {participant} ({participants_processed}/{total_participants})\")\n        sys.stdout.flush()\n    print(\"\\nFinished parsing connectivity data.\\n\")\n    return parsed_data\n\n\ndef generate_within_network_edge_labels(networks):\n    \"\"\"Generate edge labels for within-network connections.\"\"\"\n    edge_labels = []\n    for network_name, regions in networks.items():\n        region_indices = [region[0] for region in regions]\n        region_names = [region[1] for region in regions]\n        n_regions = len(region_indices)\n        for i in range(n_regions):\n            for j in range(i + 1, n_regions):\n                edge_labels.append((region_names[i], region_names[j]))\n    return edge_labels\n\n\ndef generate_between_network_edge_labels(networks):\n    \"\"\"Generate edge labels for between-network connections.\"\"\"\n    edge_labels = []\n    network_names = sorted(list(networks.keys()))\n    for i in range(len(network_names)):\n        for j in range(i + 1, len(network_names)):\n            net1, net2 = network_names[i], network_names[j]\n            regions1 = networks[net1]\n            regions2 = networks[net2]\n            for region1 in regions1:\n                for region2 in regions2:\n                    edge_labels.append((region1[1], region2[1]))\n    return edge_labels\n\n\ndef standardize_edge(edge):\n    \"\"\"Ensure that the edge tuple is always in a consistent order.\"\"\"\n    return tuple(sorted(edge))\n\n\ndef generate_region_pairs(labels):\n    \"\"\"Generate all possible region pairs.\"\"\"\n    region_pairs = []\n    for i in range(len(labels)):\n        for j in range(i + 1, len(labels)):\n            region_pairs.append((labels[i], labels[j]))\n    return region_pairs\n\n\ndef should_include_edge(diff, corr1, corr2, change_type):\n    \"\"\"Determine whether a connection should be included based on the change type.\"\"\"\n    if change_type == 'IMPC':\n        return (diff > 0) and (corr1 > 0) and (corr2 > 0)\n    elif change_type == 'DMPC':\n        return (diff < 0) and (corr1 > 0) and (corr2 > 0)\n    elif change_type == 'SNPC':\n        return (corr1 < 0) and (corr2 > 0)\n    elif change_type == 'SPNC':\n        return (corr1 > 0) and (corr2 < 0)\n    elif change_type == 'IMNC':\n        return (diff < 0) and (corr1 < 0) and (corr2 < 0)\n    elif change_type == 'DMNC':\n        return (diff > 0) and (corr1 < 0) and (corr2 < 0)\n    return False\n\n\ndef determine_favored_group(diff, corr1, corr2, change_type, comparison):\n    \"\"\"Determine which group is favored based on the change type and correlation values.\"\"\"\n    group1_name, group2_name = group_names[comparison]\n    if change_type == 'IMPC':\n        return f'{group1_name} favored' if (diff > 0 and corr1 > 0 and corr2 > 0) else f'{group2_name} favored'\n    elif change_type == 'DMPC':\n        return f'{group1_name} favored' if (diff < 0 and corr1 > 0 and corr2 > 0) else f'{group2_name} favored'\n    elif change_type == 'SNPC':\n        return f'{group2_name} favored' if (corr1 < 0 and corr2 > 0) else f'{group1_name} favored'\n    elif change_type == 'IMNC':\n        return f'{group1_name} favored' if (diff < 0 and corr1 < 0 and corr2 < 0) else f'{group2_name} favored'\n    elif change_type == 'DMNC':\n        return f'{group1_name} favored' if (diff > 0 and corr1 < 0 and corr2 < 0) else f'{group2_name} favored'\n    elif change_type == 'SPNC':\n        return f'{group2_name} favored' if (corr1 > 0 and corr2 < 0) else f'{group1_name} favored'\n    return None\n\n# =============================================================================\n# 4. Data Loading Functions\n# =============================================================================\ndef load_connectivity_data(base_dir, participants, modes):\n    \"\"\"Load raw connectivity matrices with comprehensive validation.\"\"\"\n    print(\"\\nLoading raw connectivity matrices...\")\n    \n    connectivity_data = {}\n    total_participants = len(participants)\n    out_of_bounds_count = 0\n    total_matrices = 0\n    \n    for i, participant in enumerate(participants, 1):\n        connectivity_data[participant] = {}\n        matrices_loaded = 0\n        \n        for mode in modes:\n            matrix_path = os.path.join(base_dir, participant, mode, \n                                     f\"{participant}_correlation_matrices.npy.npz\")\n            \n            if os.path.exists(matrix_path):\n                try:\n                    data = np.load(matrix_path, allow_pickle=True)\n                    connectivity_data[participant][mode] = {}\n                    \n                    for key in data.files:\n                        matrix = data[key]\n                        if matrix.ndim != 2:\n                            print(f\"Warning: Non-2D matrix found in {matrix_path}, key {key}\")\n                            continue\n                        matrix = matrix[:-2, :-2]\n                        if np.any(matrix < -1) or np.any(matrix > 1):\n                            out_of_bounds_count += 1\n                            print(f\"\\nOut-of-bounds values in {participant}, {mode}, {key}\")\n                            print(f\"Range: [{np.min(matrix):.3f}, {np.max(matrix):.3f}]\")\n                            matrix = np.clip(matrix, -1, 1)\n                        if np.isnan(matrix).any() or np.isinf(matrix).any():\n                            print(f\"\\nWarning: NaN/Inf values in {participant}, {mode}, {key}\")\n                            valid_mask = ~(np.isnan(matrix) | np.isinf(matrix))\n                            if valid_mask.any():\n                                mean_val = matrix[valid_mask].mean()\n                                matrix = np.nan_to_num(matrix, nan=mean_val, posinf=mean_val, neginf=mean_val)\n                            else:\n                                print(f\"Error: No valid values in matrix for {participant}, {mode}, {key}\")\n                                continue\n                        \n                        try:\n                            state, window = map(int, key.split('_'))\n                            connectivity_data[participant][mode][(state, window)] = matrix\n                            matrices_loaded += 1\n                            total_matrices += 1\n                        except ValueError:\n                            print(f\"Warning: Invalid key format {key} in {matrix_path}\")\n                            continue\n                            \n                except Exception as e:\n                    print(f\"\\nError loading {matrix_path}: {str(e)}\")\n                    continue\n        \n        sys.stdout.write(f\"\\rLoading matrices: {i}/{total_participants} participants | \"\n                        f\"Current: {participant} | Matrices loaded: {matrices_loaded}\")\n        sys.stdout.flush()\n    \n    print(f\"\\nFinished loading {total_matrices} matrices.\")\n    print(f\"Out-of-bounds matrices encountered: {out_of_bounds_count}\")\n    print(f\"Out-of-bounds percentage: {(out_of_bounds_count/total_matrices*100):.2f}%\")\n    \n    return connectivity_data\n\n# =============================================================================\n# 5. Connectivity Calculation Functions\n# =============================================================================\ndef calculate_network_connectivity(connectivity_data, networks, labels):\n    \"\"\"Calculate within and between network connectivity from raw matrices.\"\"\"\n    print(\"\\nCalculating network connectivity...\")\n    \n    within_network_connectivity = {}\n    between_network_connectivity = {}\n    networks_indices = {k: [r[0] for r in v] for k, v in networks.items()}\n    \n    total_participants = len(connectivity_data)\n    processed = 0\n    \n    for participant, modes_data in connectivity_data.items():\n        for mode, windows_data in modes_data.items():\n            for (state, window), matrix in windows_data.items():\n                key = (participant, mode, (state, window))\n                within_network_connectivity[key] = []\n                between_network_connectivity[key] = []\n                \n                for network_name, regions in networks.items():\n                    indices = networks_indices[network_name]\n                    network_matrix = matrix[np.ix_(indices, indices)]\n                    mask = np.triu_indices_from(network_matrix, k=1)\n                    \n                    for idx in range(len(mask[0])):\n                        i, j = mask[0][idx], mask[1][idx]\n                        region1 = regions[i][1]\n                        region2 = regions[j][1]\n                        conn_value = network_matrix[i, j]\n                        within_network_connectivity[key].append(\n                            f\"[{network_name}]: {region1} - {conn_value:.2f} - {region2}\")\n                \n                network_names = sorted(networks.keys())\n                for i in range(len(network_names)):\n                    for j in range(i + 1, len(network_names)):\n                        net1, net2 = network_names[i], network_names[j]\n                        indices1 = networks_indices[net1]\n                        indices2 = networks_indices[net2]\n                        between_matrix = matrix[np.ix_(indices1, indices2)]\n                        \n                        regions1 = networks[net1]\n                        regions2 = networks[net2]\n                        \n                        for ii in range(len(indices1)):\n                            for jj in range(len(indices2)):\n                                region1 = regions1[ii][1]\n                                region2 = regions2[jj][1]\n                                conn_value = between_matrix[ii, jj]\n                                between_network_connectivity[key].append(\n                                    f\"[{net1}, {net2}]: {region1} - {conn_value:.2f} - {region2}\")\n        \n        processed += 1\n        sys.stdout.write(f\"\\rProcessing: {processed}/{total_participants} participants | \"\n                        f\"Current: {participant}\")\n        sys.stdout.flush()\n    \n    sys.stdout.write(\"\\nFinished calculating network connectivity.\\n\")\n    sys.stdout.flush()\n    \n    return within_network_connectivity, between_network_connectivity\n\n\ndef organize_connectivity_data(participant_groups, modes, parsed_connectivity, participants):\n    \"\"\"\n    Organize connectivity data for NBS analysis using parsed connectivity data.\n\n    Args:\n        participant_groups: Dictionary mapping participants to their groups.\n        modes: List of modes (e.g., ['EC', 'EO']).\n        parsed_connectivity: Parsed connectivity data (within or between).\n        participants: List of all participants.\n\n    Returns:\n        Dictionary of organized data ready for NBS analysis.\n    \"\"\"\n    print(\"\\nOrganizing connectivity data for NBS analysis...\")\n    organized_data = {group: {mode: {} for mode in modes} for group in set(participant_groups.values())}\n\n    total_participants = len(set(key[0] for key in parsed_connectivity.keys()))\n    participants_processed = 0\n\n    participant_keys = {}\n    for key in parsed_connectivity.keys():\n        participant = key[0]\n        if participant not in participant_keys:\n            participant_keys[participant] = []\n        participant_keys[participant].append(key)\n\n    for participant in participants:\n        group = participant_groups.get(participant)\n        if participant not in participant_keys or not group:\n            continue\n        for mode in modes:\n            for key in participant_keys[participant]:\n                if key[1] != mode:\n                    continue\n                _, _, (state, window) = key\n                connections = parsed_connectivity[key]\n                connections.sort(key=lambda x: (x[1], x[3]))\n                vector_form = [conn[2] for conn in connections]\n                state_window = f\"state_{state}_window_{window}\"\n                if state_window not in organized_data[group][mode]:\n                    organized_data[group][mode][state_window] = {}\n                organized_data[group][mode][state_window][participant] = np.array(vector_form)\n        participants_processed += 1\n        sys.stdout.write(f\"\\rOrganized data for participant {participant} ({participants_processed}/{total_participants})\")\n        sys.stdout.flush()\n\n    print(\"\\nFinished organizing connectivity data.\\n\")\n    return organized_data\n\n# =============================================================================\n# 6. Permutation and Statistical Functions\n# =============================================================================\ndef process_single_permutation(combined_data, n1, threshold):\n    \"\"\"Process a single permutation.\"\"\"\n    np.random.shuffle(combined_data)  # In-place shuffle\n    perm_data1 = combined_data[:n1]\n    perm_data2 = combined_data[n1:]\n    \n    t_stats, _, _, _, _ = compute_test_stats(perm_data1, perm_data2)\n    significant_edges = np.abs(t_stats) > threshold\n    n_edges = len(t_stats)\n    n_nodes = int((1 + np.sqrt(1 + 8 * n_edges)) / 2)\n    \n    adj_matrix = np.zeros((n_nodes, n_nodes))\n    edge_idx = 0\n    for i in range(n_nodes):\n        for j in range(i + 1, n_nodes):\n            if significant_edges[edge_idx]:\n                adj_matrix[i, j] = 1\n                adj_matrix[j, i] = 1\n            edge_idx += 1\n    \n    G = nx.from_numpy_array(adj_matrix)\n    components = list(nx.connected_components(G))\n    \n    return max([len(comp) for comp in components]) if components else 0\n\n\ndef compute_test_stats(data1, data2):\n    \"\"\"Optimized test statistic computation using NumPy operations.\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=RuntimeWarning, message='Degrees of freedom <= 0 for slice')\n        warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in divide')\n        \n        n1 = len(data1)\n        n2 = len(data2)\n        \n        mean1 = np.mean(data1, axis=0)\n        mean2 = np.mean(data2, axis=0)\n        mean_diff = mean1 - mean2\n        \n        var1 = np.var(data1, axis=0, ddof=1) if n1 > 1 else np.zeros_like(mean1)\n        var2 = np.var(data2, axis=0, ddof=1) if n2 > 1 else np.zeros_like(mean2)\n        \n        pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / max(n1 + n2 - 2, 1)\n        pooled_std = np.sqrt(pooled_var * (1/n1 + 1/n2))\n        \n        t_stats = np.zeros_like(mean_diff)\n        mask = (pooled_std > 0) & (~np.isnan(pooled_std))\n        t_stats[mask] = mean_diff[mask] / pooled_std[mask]\n        \n        cohens_d = np.zeros_like(mean_diff)\n        pooled_sd = np.sqrt((var1 + var2) / 2)\n        mask = (pooled_sd > 0) & (~np.isnan(pooled_sd))\n        cohens_d[mask] = mean_diff[mask] / pooled_sd[mask]\n        \n        return t_stats, mean_diff, var1, var2, cohens_d\n\n# =============================================================================\n# 7. Network Based Statistic (NBS) Class and Methods\n# =============================================================================\nclass NetworkBasedStatistic:\n    \"\"\"Optimized implementation of Network-Based Statistic.\"\"\"\n    \n    def __init__(self, \n                 max_permutations: int = 5000,\n                 primary_threshold: float = 1.5,\n                 convergence_window: int = 50,\n                 convergence_alpha: float = 0.05,\n                 min_iterations: int = 100,\n                 window_idx: int = None,\n                 total_windows: int = None,\n                 state_window: str = None):\n        \"\"\"\n        Initialize NBS with parallel processing capability and window tracking.\n        \"\"\"\n        self.max_permutations = max_permutations\n        self.primary_threshold = primary_threshold\n        self.convergence_stats = ConvergenceStats(\n            window_size=convergence_window,\n            alpha=convergence_alpha,\n            min_iterations=min_iterations\n        )\n        self.window_idx = window_idx\n        self.total_windows = total_windows\n        self.state_window = state_window\n        \n        if self.window_idx is not None and self.total_windows is not None:\n            logging.info(f\"\\nStarting window {self.window_idx}/{self.total_windows}: {self.state_window}\")\n    \n    def _empty_result(self):\n        \"\"\"Return an empty result dictionary when no significant components are found.\"\"\"\n        return {\n            'significant_components': [],\n            'component_pvals': [],\n            'effect_sizes': [],\n            'component_edges': [],\n            'n_nodes': 0,\n            'current_permutation': 0,\n            'convergence_info': {\n                'n_permutations': 0,\n                'converged': False,\n                'running_mean': [],\n                'running_var': []\n            }\n        }\n\n    def _process_chunk(self, chunk_size, combined_data, n1):\n        \"\"\"Process a chunk of permutations.\"\"\"\n        results = []\n        for _ in range(chunk_size):\n            perm_data = combined_data.copy()\n            result = process_single_permutation(perm_data, n1, self.primary_threshold)\n            results.append(result)\n        return results\n\n    def _find_components(self, adjacency_matrix: np.ndarray) -> Tuple[List[set], List[int], List[float], List[List[Tuple[int, int]]]]:\n        \"\"\"Identify and characterize connected components.\"\"\"\n        if adjacency_matrix.size == 0 or not np.any(adjacency_matrix):\n            return [], [], [], []\n\n        try:\n            G = nx.from_numpy_array(adjacency_matrix)\n            components = list(nx.connected_components(G))\n            \n            if not components:\n                return [], [], [], []\n                \n            sizes = []\n            densities = []\n            component_edges = []\n\n            for comp in components:\n                if not comp:\n                    continue\n                    \n                subgraph = G.subgraph(comp).copy()\n                \n                if subgraph.number_of_edges() > 0:\n                    sizes.append(len(comp))\n                    densities.append(nx.density(subgraph))\n                    component_edges.append(list(subgraph.edges()))\n\n            return components, sizes, densities, component_edges\n        except Exception as e:\n            logging.error(f\"Error in finding components: {str(e)}\")\n            return [], [], [], []\n\n        \n    def _calculate_final_results(self, orig_components, perm_max_sizes, t_stats, orig_effects, mean_diff, \n                                   adj_matrix, G, n_nodes, processed_perms):\n        \"\"\"Calculate final results from permutation testing.\"\"\"\n        significant_components = []\n        component_pvals = []\n        component_effect_sizes = []\n        component_edges_list = []\n        component_all_connections = []\n\n        for component in orig_components:\n            if len(component) > 1:\n                subgraph = G.subgraph(component).copy()\n                size = len(component)\n                n_greater = np.sum(np.array(perm_max_sizes) >= size)\n                p_value = n_greater / len(perm_max_sizes) if len(perm_max_sizes) > 0 else 1.0\n\n                if p_value < 0.05:\n                    all_connections = []\n                    nodes = list(component)\n                    for i in range(len(nodes)):\n                        for j in range(i + 1, len(nodes)):\n                            all_connections.append((nodes[i], nodes[j]))\n\n                    significant_components.append(component)\n                    component_pvals.append(p_value)\n                    component_edges_list.append(list(subgraph.edges()))\n                    component_all_connections.append(all_connections)\n\n                    component_effect_sizes.append({\n                        'cohens_d': np.mean([orig_effects[i] for i in component]),\n                        't_stat': np.mean([t_stats[i] for i in component]),\n                        'density': nx.density(subgraph),\n                        'size': size,\n                        'mean_effect': np.mean([mean_diff[i] for i in component])\n                    })\n\n        return {\n            'significant_components': significant_components,\n            'component_pvals': component_pvals,\n            'effect_sizes': component_effect_sizes,\n            'component_edges': component_edges_list,\n            'component_all_connections': component_all_connections,\n            'n_nodes': n_nodes,\n            'current_permutation': processed_perms,\n            'convergence_info': {\n                'n_permutations': len(perm_max_sizes),\n                'converged': self.convergence_stats.converged,\n                'running_mean': self.convergence_stats.running_mean,\n                'running_var': self.convergence_stats.running_var\n            }\n        }\n\n    def fit(self, data1: np.ndarray, data2: np.ndarray) -> Dict:\n        \"\"\"Optimized fitting using ThreadPoolExecutor.\"\"\"\n        n1, n_edges = data1.shape\n        n2 = data2.shape[0]\n\n        if n1 == 0 or n2 == 0:\n            return self._empty_result()\n\n        t_stats, mean_diff, var1, var2, cohens_d = compute_test_stats(data1, data2)\n        significant_edges = np.abs(t_stats) > self.primary_threshold\n        if not np.any(significant_edges):\n            return self._empty_result()\n\n        n_nodes = int((1 + np.sqrt(1 + 8 * n_edges)) / 2)\n        edge_indices = np.triu_indices(n_nodes, k=1)\n\n        adj_matrix = np.zeros((n_nodes, n_nodes))\n        adj_matrix[edge_indices[0][significant_edges], edge_indices[1][significant_edges]] = 1\n        adj_matrix = adj_matrix + adj_matrix.T\n\n        G = nx.from_numpy_array(adj_matrix)\n        orig_components = list(nx.connected_components(G))\n\n        if not orig_components:\n            return self._empty_result()\n\n        combined_data = np.vstack([data1, data2])\n        chunk_size = 50\n        perm_max_sizes = []\n        processed = 0\n\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = []\n            remaining_perms = self.max_permutations\n\n            if self.window_idx is not None:\n                sys.stdout.write('\\r' + ' ' * 100)\n                sys.stdout.flush()\n\n            while remaining_perms > 0 and not self.convergence_stats.converged:\n                current_chunk = min(chunk_size, remaining_perms)\n                future = executor.submit(\n                    self._process_chunk,\n                    current_chunk,\n                    combined_data,\n                    n1\n                )\n                futures.append(future)\n                remaining_perms -= current_chunk\n\n                for completed_future in concurrent.futures.as_completed(futures):\n                    chunk_results = completed_future.result()\n                    perm_max_sizes.extend(chunk_results)\n                    processed += len(chunk_results)\n\n                    if self.window_idx is not None:\n                        sys.stdout.write('\\r' + ' ' * 100)\n                        sys.stdout.write('\\r')\n                        progress_msg = (f\"Processing window {self.window_idx}/{self.total_windows}: {self.state_window} | \"\n                                      f\"Matrix shape: {n1}x{n_edges} | Threshold: {self.primary_threshold:.3f} | \"\n                                      f\"Permutations: {processed}/{self.max_permutations}\")\n                        sys.stdout.write(progress_msg)\n                        sys.stdout.flush()\n\n                    if any(self.convergence_stats.update(size) for size in chunk_results):\n                        remaining_perms = 0\n                        break\n\n                futures = []\n\n            if self.window_idx is not None:\n                sys.stdout.write('\\n')\n                sys.stdout.flush()\n\n        return self._calculate_final_results(\n            orig_components, perm_max_sizes, t_stats, cohens_d, mean_diff, \n            adj_matrix, G, n_nodes, processed\n        )\n\n# =============================================================================\n# 8. Dynamic Group Differences Calculation Functions\n# =============================================================================\ndef calculate_dynamic_group_differences_nbs(\n    group1_data, group2_data, comparison, edge_labels, alpha=0.05, \n    state_window_fraction=1.0, max_permutations=5000, group3_data=None,\n    results_filename=None,\n    existing_differences=None,\n    existing_nbs_results=None,\n    existing_group1_means_all=None,\n    existing_group2_means_all=None,\n    existing_convergence_stats=None,\n    existing_total_state_windows=0,\n    existing_group3_means_all=None,\n    processed_windows=None\n):\n    \"\"\"Calculate group differences using Network-Based Statistics with global threshold.\"\"\"\n    log_filename = setup_logging()\n    logging.info(\"Starting NBS-based dynamic group differences calculation...\")\n\n    differences = existing_differences if existing_differences is not None else {}\n    nbs_results = existing_nbs_results if existing_nbs_results is not None else {}\n    group1_means_all = existing_group1_means_all if existing_group1_means_all is not None else {}\n    group2_means_all = existing_group2_means_all if existing_group2_means_all is not None else {}\n    convergence_stats = existing_convergence_stats if existing_convergence_stats is not None else {}\n    total_state_windows = existing_total_state_windows\n    processed_windows = processed_windows if processed_windows is not None else set()\n\n    if group3_data is not None:\n        group3_means_all = existing_group3_means_all if existing_group3_means_all is not None else {}\n    else:\n        group3_means_all = None\n\n    modes = list(group1_data.keys())\n    total_modes = len(modes)\n\n    logging.info(\"\\nCollecting data across all modes for global threshold optimization...\")\n    data1_all_windows = []\n    data2_all_windows = []\n    total_windows = 0\n\n    for mode in modes:\n        group1_mode_data = group1_data[mode]\n        group2_mode_data = group2_data[mode]\n        common_windows = set(group1_mode_data.keys()) & set(group2_mode_data.keys())\n        total_windows += len(common_windows)\n        for state_window in common_windows:\n            group1_arrays = [participant_data for participant_data in group1_mode_data[state_window].values()]\n            group2_arrays = [participant_data for participant_data in group2_mode_data[state_window].values()]\n            \n            if all(arr.shape == group1_arrays[0].shape for arr in group1_arrays + group2_arrays):\n                data1_all_windows.append(np.array(group1_arrays))\n                data2_all_windows.append(np.array(group2_arrays))\n\n    logging.info(f\"Total windows across all modes: {total_windows}\")\n\n    threshold_filename = f'optimized_threshold_{comparison}.pkl'\n    try:\n        with open(threshold_filename, 'rb') as f:\n            optimal_threshold = pickle.load(f)\n        logging.info(f\"\\nLoaded optimized threshold: {optimal_threshold:.3f} for comparison: {comparison}\")\n    except FileNotFoundError:\n        logging.info(\"Calculating global optimal threshold...\")\n        optimal_threshold = optimize_global_threshold(data1_all_windows, data2_all_windows, total_windows)\n        logging.info(f\"\\nUsing global threshold: {optimal_threshold:.3f} for all windows\")\n        with open(threshold_filename, 'wb') as f:\n            pickle.dump(optimal_threshold, f)\n        logging.info(f\"Optimized threshold saved to {threshold_filename}\")\n\n    logging.info(\"\\nStarting mode-specific processing with the optimized threshold...\")\n\n    import sys\n\n    for mode_idx, mode in enumerate(modes, 1):\n        mode_message = f\"\\nProcessing mode: {mode} [{mode_idx}/{total_modes}] for comparison: {comparison}\"\n        logging.info(mode_message)\n        print(mode_message)\n\n        group1_mode_data = group1_data[mode]\n        group2_mode_data = group2_data[mode]\n\n        common_windows = set(group1_mode_data.keys()) & set(group2_mode_data.keys())\n\n        if group3_data is not None:\n            group3_mode_data = group3_data[mode]\n            common_windows = common_windows & set(group3_mode_data.keys())\n\n        state_windows = sorted(common_windows, key=lambda x: (int(x.split('_')[1]), int(x.split('_')[3])))\n\n        total_mode_windows = len(state_windows)\n\n        if state_window_fraction < 1.0:\n            num_windows = max(1, int(total_mode_windows * state_window_fraction))\n            state_windows = random.sample(state_windows, num_windows)\n            logging.info(f\"Analyzing {num_windows}/{total_mode_windows} windows for {mode}\")\n        else:\n            logging.info(f\"Analyzing {total_mode_windows} windows for {mode}\")\n        \n        for window_idx, state_window in enumerate(state_windows, 1):\n            key = (mode, state_window)\n            if key in processed_windows:\n                continue\n            progress_message = f\"Processing window {window_idx}/{len(state_windows)}: {state_window} | \"\n            sys.stdout.write('\\r' + progress_message)\n            sys.stdout.flush()\n\n            logging.info(f\"\\nStarting window {window_idx}/{len(state_windows)}: {state_window}\")\n            \n            group1_arrays = [participant_data for participant_data in group1_mode_data[state_window].values()]\n            group2_arrays = [participant_data for participant_data in group2_mode_data[state_window].values()]\n\n            if group3_data is not None:\n                group3_mode_data = group3_data[mode]\n                group3_arrays = [participant_data for participant_data in group3_mode_data[state_window].values()]\n            else:\n                group3_arrays = None\n\n            if not all(arr.shape == group1_arrays[0].shape for arr in group1_arrays + group2_arrays):\n                warning_msg = f\"Skipping window {state_window} - Inconsistent array shapes\"\n                logging.warning(warning_msg)\n                continue\n\n            group1_values = np.array(group1_arrays)\n            group2_values = np.array(group2_arrays)\n            if group3_arrays is not None:\n                group3_values = np.array(group3_arrays)\n\n            matrix_shape_str = f\"Matrix shape: {group1_values.shape[0]}x{group1_values.shape[1]} | \"\n            progress_message += matrix_shape_str\n            sys.stdout.write('\\r' + progress_message)\n            sys.stdout.flush()\n\n            threshold_str = f\"Threshold: {optimal_threshold:.3f} | \"\n            progress_message += threshold_str\n            sys.stdout.write('\\r' + progress_message)\n            sys.stdout.flush()\n            \n            nbs = NetworkBasedStatistic(\n                max_permutations=max_permutations,\n                primary_threshold=optimal_threshold,\n                convergence_window=50,\n                convergence_alpha=0.05,\n                min_iterations=100,\n                window_idx=window_idx,\n                total_windows=len(state_windows),\n                state_window=state_window\n            )\n\n            result = nbs.fit(group1_values, group2_values)\n\n            n_components = len(result['significant_components']) if result['significant_components'] else 0\n            current_perm = result.get('current_permutation', 0)\n\n            perm_comp_str = f\"Permutations: {current_perm}/{max_permutations} | Components: {n_components}\"\n            progress_message += perm_comp_str\n            sys.stdout.write('\\r' + progress_message)\n            sys.stdout.flush()\n\n            print()\n            logging.info(f\"Completed window {window_idx}/{len(state_windows)}: {state_window} | \"\n                         f\"Permutations: {current_perm}/{max_permutations} | Components: {n_components}\")\n\n            if result['significant_components']:\n                key = (mode, state_window)\n                nbs_results[key] = result\n                convergence_stats[key] = result['convergence_info']\n\n                group1_means = np.mean(group1_values, axis=0)\n                group2_means = np.mean(group2_values, axis=0)\n                diff_values = group1_means - group2_means\n\n                edge_indices = range(len(diff_values))\n                differences[key] = dict(zip(edge_indices, diff_values))\n                group1_means_all[key] = dict(zip(edge_indices, group1_means))\n                group2_means_all[key] = dict(zip(edge_indices, group2_means))\n\n                if group3_arrays is not None:\n                    group3_means = np.mean(group3_values, axis=0)\n                    group3_means_all[key] = dict(zip(edge_indices, group3_means))\n\n                total_state_windows += 1\n                logging.info(f\"Found {n_components} significant components for {state_window}\")\n\n            processed_windows.add(key)\n            if results_filename is not None:\n                try:\n                    with open(results_filename, 'wb') as f:\n                        if group3_data is not None:\n                            pickle.dump((differences, nbs_results, group1_means_all, group2_means_all,\n                                         convergence_stats, total_state_windows, group3_means_all, processed_windows), f)\n                        else:\n                            pickle.dump((differences, nbs_results, group1_means_all, group2_means_all,\n                                         convergence_stats, total_state_windows, processed_windows), f)\n                    logging.info(f\"Intermediate results saved to {results_filename}\")\n                except Exception as e:\n                    logging.error(f\"Error saving intermediate results: {str(e)}\")\n\n        final_msg = f\"\\nAnalysis complete for mode {mode}. Total windows with significant components: {total_state_windows}\"\n        logging.info(final_msg)\n        print(final_msg)\n\n    logging.info(f\"\\nAnalysis complete. Total windows with significant components: {total_state_windows}\")\n    logging.info(f\"Complete log available in: {log_filename}\")\n\n    print(f\"\\nAnalysis complete. Total windows with significant components: {total_state_windows}\")\n    print(f\"Complete log available in: {log_filename}\")\n\n    if group3_data is not None:\n        return (differences, nbs_results, group1_means_all, group2_means_all, \n                convergence_stats, total_state_windows, group3_means_all)\n    else:\n        return (differences, nbs_results, group1_means_all, group2_means_all, \n                convergence_stats, total_state_windows)\n\n# =============================================================================\n# 9. Logging and Threshold Optimization Functions\n# =============================================================================\ndef setup_logging():\n    \"\"\"Set up logging configuration.\"\"\"\n    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n    log_filename = f'nbs_analysis_{timestamp}.log'\n    \n    f_handler = logging.FileHandler(log_filename)\n    f_handler.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(message)s')\n    f_handler.setFormatter(formatter)\n    \n    logger = logging.getLogger()\n    logger.handlers = []\n    logger.addHandler(f_handler)\n    logger.setLevel(logging.INFO)\n\n    return log_filename\n\n\ndef optimize_global_threshold(data1_all_windows: List[np.ndarray], \n                              data2_all_windows: List[np.ndarray],\n                              total_windows: int,\n                              thresholds: list = [1.5, 2.0, 2.5, 3.0]) -> float:\n    \"\"\"\n    Optimize the primary t-statistic threshold for NBS analysis using stability analysis\n    across all windows.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=RuntimeWarning)\n        stability_scores = []\n        \n        total_thresholds = len(thresholds)\n        total_windows = len(data1_all_windows)\n        total_iterations = total_thresholds * total_windows\n        current_iteration = 0\n\n        for threshold_idx, threshold in enumerate(thresholds):\n            all_windows_components = []\n            \n            for window_idx in range(len(data1_all_windows)):\n                data1 = data1_all_windows[window_idx]\n                data2 = data2_all_windows[window_idx]\n                \n                components_list = []\n                n1, n_edges = data1.shape\n                n2 = data2.shape[0]\n                \n                num_bootstrap_iterations = 20\n                bootstrap_indices = [(\n                    np.random.choice(n1, size=n1, replace=True),\n                    np.random.choice(n2, size=n2, replace=True)\n                ) for _ in range(num_bootstrap_iterations)]\n                \n                for idx1, idx2 in bootstrap_indices:\n                    data1_bootstrap = data1[idx1]\n                    data2_bootstrap = data2[idx2]\n                    \n                    mean_diff = np.mean(data1_bootstrap, axis=0) - np.mean(data2_bootstrap, axis=0)\n                    var1 = np.var(data1_bootstrap, axis=0, ddof=1)\n                    var2 = np.var(data2_bootstrap, axis=0, ddof=1)\n                    pooled_var = ((len(idx1) - 1) * var1 + (len(idx2) - 1) * var2) / (len(idx1) + len(idx2) - 2)\n                    pooled_std = np.sqrt(pooled_var * (1/len(idx1) + 1/len(idx2)))\n                    \n                    valid_mask = pooled_std > 0\n                    t_stats = np.zeros(n_edges)\n                    t_stats[valid_mask] = mean_diff[valid_mask] / pooled_std[valid_mask]\n                    \n                    n_nodes = int((1 + np.sqrt(1 + 8 * n_edges)) / 2)\n                    adj_matrix = np.zeros((n_nodes, n_nodes))\n                    edge_indices = np.triu_indices(n_nodes, k=1)\n                    significant_edges = np.abs(t_stats) > threshold\n                    adj_matrix[edge_indices[0][significant_edges], edge_indices[1][significant_edges]] = 1\n                    \n                    G = nx.from_numpy_array(adj_matrix)\n                    components = list(nx.connected_components(G))\n                    components_list.append(len(components))\n                \n                all_windows_components.extend(components_list)\n                current_iteration += 1\n                progress = (current_iteration / total_iterations) * 100\n                sys.stdout.write(f\"\\rOptimizing threshold: {progress:.2f}% complete\")\n                sys.stdout.flush()\n            \n            stability_scores.append(np.var(all_windows_components))\n        \n        sys.stdout.write('\\n')\n\n    optimal_idx = np.argmin(stability_scores)\n    optimal_threshold = thresholds[optimal_idx]\n    \n    print(f\"Selected global threshold: {optimal_threshold:.3f}\")\n    return optimal_threshold\n\n# =============================================================================\n# 10. Summary and Excel Output Functions\n# =============================================================================\ndef quantification_summary_to_excel_nbs(\n    differences,\n    nbs_results,\n    group1_means_all,\n    group2_means_all,\n    edge_labels,\n    networks,\n    labels,\n    change_type,\n    comparison,\n    writer=None,\n    group3_means_all=None,\n    alpha=0.05,\n    existing_all_dfs=None,\n    existing_summary_data=None,\n    processed_keys=None,\n    summary_filename=None,\n    edge_indices_reverse=None\n):\n    \"\"\"Generate summary with proper network pair extraction, including weighted values.\"\"\"\n    all_dfs = existing_all_dfs if existing_all_dfs is not None else []\n    processed_keys = processed_keys if processed_keys is not None else set()\n    summary_list = []\n\n    network_names = sorted(list(networks.keys()))\n    categories = [\n        'Overall',\n        'Intra-Network',\n        'Inter-Network',\n    ]\n    intra_network_categories = [f'Intra-Network ({net})' for net in network_names]\n    categories.extend(intra_network_categories)\n\n    inter_network_pairs = []\n    for i in range(len(network_names)):\n        for j in range(i + 1, len(network_names)):\n            net1, net2 = sorted([network_names[i], network_names[j]])\n            inter_network_pairs.append(f'{net1}-{net2}')\n\n    inter_network_categories = [f'Inter-Network ({pair})' for pair in inter_network_pairs]\n    categories.extend(inter_network_categories)\n    \n    unique_state_windows = len(set([key[1] for key in nbs_results.keys()]))\n\n    summary_data = existing_summary_data if existing_summary_data is not None else {}\n    for category in categories:\n        if category not in summary_data:\n            summary_data[category] = {\n                'Connection Count': 0,\n                'Total Connection Count': 0,\n                'Connection Percentage': 0,\n                'Mean_Component_Size': [],\n                'Mean_Component_Density': [],\n                'Mean_Effect_Size': [],\n                'Mean_T_Statistic': [],\n                'Mean_P_Value': [],\n                f'{group_names[comparison][0]} favored': 0,\n                f'{group_names[comparison][1]} favored': 0,\n                f'{group_names[comparison][0]} weighted value (average)': 0,\n                f'{group_names[comparison][1]} weighted value (average)': 0,\n                f'{group_names[comparison][0]} weighted value (total)': 0,\n                f'{group_names[comparison][1]} weighted value (total)': 0\n            }\n            if comparison == 'oac_vs_tcoa' and group3_means_all is not None:\n                summary_data[category].update({\n                    'Compensatory_Connections': 0,\n                    'Deterioration_Connections': 0,\n                    'Restoration_Count': 0,\n                    'Alternative_Count': 0,\n                    'Normalization_Count': 0,\n                    'Enhancement_Count': 0,\n                    'Deterioration_Count': 0,\n                    'Maladaptive_Count': 0,\n                    'Exacerbation_Count': 0,\n                    'Decompensation_Count': 0\n                })\n\n    n_regions = len(labels)\n    total_possible_overall = (n_regions * (n_regions - 1)) // 2\n    summary_data['Overall']['Total Connection Count'] = total_possible_overall * unique_state_windows\n\n    total_possible_intra = 0\n    for net in network_names:\n        n = len(networks[net])\n        total_intra = n * (n - 1) // 2\n        total_possible_intra += total_intra\n        category = f'Intra-Network ({net})'\n        summary_data[category]['Total Connection Count'] = total_intra * unique_state_windows\n\n    summary_data['Intra-Network']['Total Connection Count'] = total_possible_intra * unique_state_windows\n    total_possible_inter = total_possible_overall - total_possible_intra\n    summary_data['Inter-Network']['Total Connection Count'] = total_possible_inter * unique_state_windows\n\n    for pair in inter_network_pairs:\n        net1, net2 = pair.split('-')\n        n1 = len(networks[net1])\n        n2 = len(networks[net2])\n        total_inter = n1 * n2\n        category = f'Inter-Network ({pair})'\n        summary_data[category]['Total Connection Count'] = total_inter * unique_state_windows\n\n    all_dfs = existing_all_dfs if existing_all_dfs is not None else []\n    processed_keys = processed_keys if processed_keys is not None else set()\n\n    keys = differences.keys()\n\n    for key in keys:\n        if key in processed_keys:\n            continue\n        mode_key, state_window = key\n        if key not in nbs_results or not nbs_results[key]['significant_components']:\n            continue\n\n        result = nbs_results[key]\n        group1_means = group1_means_all[key]\n        group2_means = group2_means_all[key]\n\n        for comp_idx, component in enumerate(result['significant_components']):\n            edges_data = []\n            all_connections = result['component_all_connections'][comp_idx]\n            \n            for connection in all_connections:\n                i, j = connection\n                edge_idx = edge_indices_reverse.get((i, j))\n                if edge_idx is None:\n                    edge_idx = edge_indices_reverse.get((j, i))\n                if edge_idx is None:\n                    continue\n\n                diff = differences[key][edge_idx]\n                g1_mean = group1_means[edge_idx]\n                g2_mean = group2_means[edge_idx]\n\n                if should_include_edge(diff, g1_mean, g2_mean, change_type):\n                    region1, region2 = labels[i], labels[j]\n                    net1 = get_network_name(region1)\n                    net2 = get_network_name(region2)\n\n                    if region1 > region2:\n                        region1, region2 = region2, region1\n                        net1, net2 = net2, net1\n\n                    network_pair = f\"{net1}-{net2}\"\n\n                    is_direct = any(edge == (i,j) or edge == (j,i) \n                                  for edge in result['component_edges'][comp_idx])\n\n                    edge_data = {\n                        'Edge': edge_idx,\n                        'Region1': region1,\n                        'Region2': region2,\n                        'Network_Pair': network_pair,\n                        'Is_Direct_Connection': is_direct,\n                        'Difference': diff,\n                        'Group1_Mean': g1_mean,\n                        'Group2_Mean': g2_mean,\n                        'Component_Size': result['effect_sizes'][comp_idx]['size'],\n                        'Component_PValue': result['component_pvals'][comp_idx],\n                        'Cohens_D': result['effect_sizes'][comp_idx]['cohens_d'],\n                        'T_Statistic': result['effect_sizes'][comp_idx]['t_stat'],\n                        'Component_Density': result['effect_sizes'][comp_idx]['density'],\n                        'State_Window': state_window,\n                        'Mode': mode_key,\n                        'Component_ID': f\"{mode_key}_{state_window}_comp_{comp_idx}\"\n                    }\n                    edges_data.append(edge_data)\n\n            if edges_data:\n                df = pd.DataFrame(edges_data)\n                df['Favored_Group'] = df.apply(\n                    lambda row: determine_favored_group(\n                        row['Difference'],\n                        row['Group1_Mean'],\n                        row['Group2_Mean'],\n                        change_type,\n                        comparison\n                    ),\n                    axis=1\n                )\n                df['Mechanism'] = None\n                if comparison == 'oac_vs_tcoa' and group3_means_all is not None:\n                    for idx, row in df.iterrows():\n                        edge_idx = row['Edge']\n                        yac_mean = group3_means_all[key][edge_idx]\n                        oac_mean = group1_means[edge_idx]\n                        tcoa_mean = group2_means[edge_idx]\n\n                        oac_distance = abs(oac_mean - yac_mean)\n                        tcoa_distance = abs(tcoa_mean - yac_mean)\n\n                        mechanism = None\n                        if tcoa_distance < oac_distance:\n                            if (tcoa_mean - yac_mean) * (oac_mean - yac_mean) > 0:\n                                mechanism = 'Restoration'\n                            else:\n                                mechanism = 'Alternative'\n                        else:\n                            if (tcoa_mean - yac_mean) * (oac_mean - yac_mean) > 0:\n                                mechanism = 'Deterioration'\n                            else:\n                                mechanism = 'Maladaptive'\n\n                        if tcoa_mean > yac_mean > oac_mean:\n                            mechanism = 'Enhancement' if tcoa_distance < oac_distance else 'Exacerbation'\n                        elif tcoa_mean < yac_mean < oac_mean:\n                            mechanism = 'Normalization' if tcoa_distance < oac_distance else 'Decompensation'\n\n                        df.at[idx, 'Mechanism'] = mechanism\n\n                    mechanism_types = {\n                        'compensatory': ['Restoration', 'Alternative', 'Enhancement', 'Normalization'],\n                        'deterioration': ['Deterioration', 'Maladaptive', 'Exacerbation', 'Decompensation']\n                    }\n                    for mech in mechanism_types['compensatory'] + mechanism_types['deterioration']:\n                        df[mech] = df['Mechanism'].apply(lambda x: 1 if x == mech else 0)\n                        \n                all_dfs.append(df)\n\n                for idx, row in df.iterrows():\n                    region1 = row['Region1']\n                    region2 = row['Region2']\n                    net1 = get_network_name(region1)\n                    net2 = get_network_name(region2)\n                    net1, net2 = sorted([net1, net2])\n\n                    categories_to_update = ['Overall']\n                    if net1 == net2:\n                        categories_to_update.extend([\n                            'Intra-Network',\n                            f'Intra-Network ({net1})'\n                        ])\n                    else:\n                        categories_to_update.extend([\n                            'Inter-Network',\n                            f'Inter-Network ({net1}-{net2})'\n                        ])\n\n                    for category in categories_to_update:\n                        data = summary_data[category]\n                        data['Connection Count'] += 1\n                        data[row['Favored_Group']] += 1\n                        data['Mean_Component_Size'].append(row['Component_Size'])\n                        data['Mean_Component_Density'].append(row['Component_Density'])\n                        data['Mean_Effect_Size'].append(row['Cohens_D'])\n                        data['Mean_T_Statistic'].append(row['T_Statistic'])\n                        data['Mean_P_Value'].append(row['Component_PValue'])\n                        group_name = row['Favored_Group'].replace(' favored', '')\n                        data[f'{group_name} weighted value (total)'] += abs(row['Difference'])\n\n                        if comparison == 'oac_vs_tcoa' and row['Mechanism']:\n                            if row['Mechanism'] in mechanism_types['compensatory']:\n                                data['Compensatory_Connections'] += 1\n                            else:\n                                data['Deterioration_Connections'] += 1\n                            data[f'{row[\"Mechanism\"]}_Count'] += 1\n\n                processed_keys.add(key)\n                if summary_filename is not None:\n                    try:\n                        with open(summary_filename, 'wb') as f:\n                            pickle.dump((all_dfs, summary_data, processed_keys), f)\n                        print(f\"Intermediate summary data saved to {summary_filename}\")\n                    except Exception as e:\n                        print(f\"Error saving intermediate summary data: {str(e)}\")\n\n                if isinstance(all_dfs, list) and len(all_dfs) > 0:\n                    final_df = pd.concat(all_dfs, ignore_index=True)\n                        \n    for category in categories:\n        data = summary_data[category]\n        total_conn = data['Total Connection Count']\n        conn_count = data['Connection Count']\n        data['Connection Percentage'] = (conn_count / total_conn) * 100 if total_conn > 0 else 0\n\n        if isinstance(final_df, pd.DataFrame) and not final_df.empty:\n            data['Direct_Connections'] = len(final_df[final_df['Is_Direct_Connection']])\n            data['Indirect_Connections'] = len(final_df[~final_df['Is_Direct_Connection']])\n        else:\n            data['Direct_Connections'] = 0\n            data['Indirect_Connections'] = 0\n\n        for key in ['Mean_Component_Size', 'Mean_Component_Density', 'Mean_Effect_Size', 'Mean_T_Statistic', 'Mean_P_Value']:\n            values = data[key]\n            data[key] = np.mean(values) if values else 0\n\n        for group in [group_names[comparison][0], group_names[comparison][1]]:\n            favored_conn = data[f'{group} favored']\n            total_weighted_value = data[f'{group} weighted value (total)']\n            data[f'{group} weighted value (average)'] = (total_weighted_value / favored_conn) if favored_conn > 0 else 0\n\n    summary_list = []\n    for category in categories:\n        data = summary_data[category]\n        summary_entry = {\n            'Category': category,\n            'Connection Count': data['Connection Count'],\n            'Total Connection Count': data['Total Connection Count'],\n            'Connection Percentage': data['Connection Percentage'],\n            'Mean_Component_Size': data['Mean_Component_Size'],\n            'Mean_Component_Density': data['Mean_Component_Density'],\n            'Mean_Effect_Size': data['Mean_Effect_Size'],\n            'Mean_T_Statistic': data['Mean_T_Statistic'],\n            'Mean_P_Value': data['Mean_P_Value'],\n            f'{group_names[comparison][0]} favored': data[f'{group_names[comparison][0]} favored'],\n            f'{group_names[comparison][1]} favored': data[f'{group_names[comparison][1]} favored'],\n            f'{group_names[comparison][0]} weighted value (average)': data[f'{group_names[comparison][0]} weighted value (average)'],\n            f'{group_names[comparison][1]} weighted value (average)': data[f'{group_names[comparison][1]} weighted value (average)'],\n            f'{group_names[comparison][0]} weighted value (total)': data[f'{group_names[comparison][0]} weighted value (total)'],\n            f'{group_names[comparison][1]} weighted value (total)': data[f'{group_names[comparison][1]} weighted value (total)']\n        }\n\n        if comparison == 'oac_vs_tcoa':\n            summary_entry.update({\n                'Compensatory_Connections': data.get('Compensatory_Connections', 0),\n                'Deterioration_Connections': data.get('Deterioration_Connections', 0),\n                'Restoration_Count': data.get('Restoration_Count', 0),\n                'Alternative_Count': data.get('Alternative_Count', 0),\n                'Normalization_Count': data.get('Normalization_Count', 0),\n                'Enhancement_Count': data.get('Enhancement_Count', 0),\n                'Deterioration_Count': data.get('Deterioration_Count', 0),\n                'Maladaptive_Count': data.get('Maladaptive_Count', 0),\n                'Exacerbation_Count': data.get('Exacerbation_Count', 0),\n                'Decompensation_Count': data.get('Decompensation_Count', 0)\n            })\n        summary_list.append(summary_entry)\n\n    try:\n        if isinstance(all_dfs, list) and len(all_dfs) > 0:\n            final_df = pd.concat(all_dfs, ignore_index=True)\n        else:\n            final_df = pd.DataFrame()\n\n        if len(summary_list) > 0:\n            df_summary_sheet = pd.DataFrame(summary_list)\n        else:\n            df_summary_sheet = pd.DataFrame()\n\n        if summary_filename is not None:\n            try:\n                with open(summary_filename, 'wb') as f:\n                    pickle.dump((all_dfs, summary_data, processed_keys), f)\n                print(f\"Intermediate summary data saved to {summary_filename}\")\n            except Exception as e:\n                print(f\"Error saving intermediate summary data: {str(e)}\")\n\n        if final_df.empty:\n            print(f\"No data to write for {comparison}, {change_type}.\")\n\n        return final_df, df_summary_sheet\n\n    except Exception as e:\n        print(f\"Error in final DataFrame creation: {str(e)}\")\n        return pd.DataFrame(), pd.DataFrame()\n\n# =============================================================================\n# 11. Global Variables\n# =============================================================================\ngroup_names = {\n    'oac_vs_yac': ('OAC', 'YAC'),\n    'oac_vs_tcoa': ('OAC', 'TCOA')\n}\n\n# =============================================================================\n# 12. Main Function and Execution Block\n# =============================================================================\ndef main():\n    start_time = time.time()\n    \n    \"\"\"\n    Main execution function with NBS implementation.\n    \"\"\"\n    print(\"Starting the NBS-based dynamic functional connectivity analysis...\")\n\n    # ---------------------------\n    # 1. Define Pilot Test or Full Analysis\n    # ---------------------------\n    pilot_test = False  # Set to False for full analysis\n\n    YACs_full = ['101', '102', '103', '104', '105', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120']\n    OACs_full = ['202', '205', '206', '207', '208', '209', '210', '211', '214', '215', '216', '217', '218', '219', '221']\n    TCOAs_full = ['401', '402', '403', '404', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416']\n\n    if pilot_test:\n        state_window_fraction = 0.2\n        max_permutations = 2500\n        YACs = YACs_full[:5]\n        OACs = OACs_full[:5]\n        TCOAs = TCOAs_full[:5]\n        \n        primary_threshold = 0.05        \n        \n        print(\"\\nPilot Testing Mode Activated:\")\n        print(f\"  YACs: {YACs}\")\n        print(f\"  OACs: {OACs}\")\n        print(f\"  TCOAs: {TCOAs}\")\n        print(f\"  State Window Fraction: {state_window_fraction}\")\n        print(f\"  Max Permutations: {max_permutations}\")\n    else:\n        state_window_fraction = 1.0\n        max_permutations = 5000\n        primary_threshold = 0.05\n        \n        YACs = YACs_full\n        OACs = OACs_full\n        TCOAs = TCOAs_full\n        print(\"\\nFull Analysis Mode Activated:\")\n        print(f\"  YACs: {YACs}\")\n        print(f\"  OACs: {OACs}\")\n        print(f\"  TCOAs: {TCOAs}\")\n        print(f\"  State Window Fraction: {state_window_fraction}\")\n        print(f\"  Max Permutations: {max_permutations}\")\n\n    participants = YACs + OACs + TCOAs\n\n    participant_groups = {p: 'YAC' for p in YACs}\n    participant_groups.update({p: 'OAC' for p in OACs})\n    participant_groups.update({p: 'TCOA' for p in TCOAs})\n\n    # ---------------------------\n    # 2. Define Labels\n    # ---------------------------\n    labels = [\n        '7Networks_LH_Cont_Cing_1-lh',\n        '7Networks_LH_Cont_Par_1-lh',\n        '7Networks_LH_Cont_PFCl_1-lh',\n        '7Networks_LH_Cont_pCun_1-lh',\n        '7Networks_LH_Default_Par_1-lh',\n        '7Networks_LH_Default_Par_2-lh',\n        '7Networks_LH_Default_pCunPCC_1-lh',\n        '7Networks_LH_Default_pCunPCC_2-lh',\n        '7Networks_LH_Default_PFC_1-lh',\n        '7Networks_LH_Default_PFC_2-lh',\n        '7Networks_LH_Default_PFC_3-lh',\n        '7Networks_LH_Default_PFC_4-lh',\n        '7Networks_LH_Default_PFC_5-lh',\n        '7Networks_LH_Default_PFC_6-lh',\n        '7Networks_LH_Default_PFC_7-lh',\n        '7Networks_LH_Default_Temp_1-lh',\n        '7Networks_LH_Default_Temp_2-lh',\n        '7Networks_LH_DorsAttn_FEF_1-lh',\n        '7Networks_LH_DorsAttn_Post_1-lh',\n        '7Networks_LH_DorsAttn_Post_2-lh',\n        '7Networks_LH_DorsAttn_Post_3-lh',\n        '7Networks_LH_DorsAttn_Post_4-lh',\n        '7Networks_LH_DorsAttn_Post_5-lh',\n        '7Networks_LH_DorsAttn_Post_6-lh',\n        '7Networks_LH_DorsAttn_PrCv_1-lh',\n        '7Networks_LH_Limbic_OFC_1-lh',\n        '7Networks_LH_Limbic_TempPole_1-lh',\n        '7Networks_LH_Limbic_TempPole_2-lh',\n        '7Networks_LH_SalVentAttn_FrOperIns_1-lh',\n        '7Networks_LH_SalVentAttn_FrOperIns_2-lh',\n        '7Networks_LH_SalVentAttn_Med_1-lh',\n        '7Networks_LH_SalVentAttn_Med_2-lh',\n        '7Networks_LH_SalVentAttn_Med_3-lh',\n        '7Networks_LH_SalVentAttn_ParOper_1-lh',\n        '7Networks_LH_SalVentAttn_PFCl_1-lh',\n        '7Networks_LH_SomMot_1-lh',\n        '7Networks_LH_SomMot_2-lh',\n        '7Networks_LH_SomMot_3-lh',\n        '7Networks_LH_SomMot_4-lh',\n        '7Networks_LH_SomMot_5-lh',\n        '7Networks_LH_SomMot_6-lh',\n        '7Networks_LH_Vis_1-lh',\n        '7Networks_LH_Vis_2-lh',\n        '7Networks_LH_Vis_3-lh',\n        '7Networks_LH_Vis_4-lh',\n        '7Networks_LH_Vis_5-lh',\n        '7Networks_LH_Vis_6-lh',\n        '7Networks_LH_Vis_7-lh',\n        '7Networks_LH_Vis_8-lh',\n        '7Networks_LH_Vis_9-lh',\n        '7Networks_RH_Cont_Cing_1-rh',\n        '7Networks_RH_Cont_Par_1-rh',\n        '7Networks_RH_Cont_Par_2-rh',\n        '7Networks_RH_Cont_PFCl_1-rh',\n        '7Networks_RH_Cont_PFCl_2-rh',\n        '7Networks_RH_Cont_PFCl_3-rh',\n        '7Networks_RH_Cont_PFCl_4-rh',\n        '7Networks_RH_Cont_PFCmp_1-rh',\n        '7Networks_RH_Cont_pCun_1-rh',\n        '7Networks_RH_Default_Par_1-rh',\n        '7Networks_RH_Default_pCunPCC_1-rh',\n        '7Networks_RH_Default_pCunPCC_2-rh',\n        '7Networks_RH_Default_PFCdPFCm_1-rh',\n        '7Networks_RH_Default_PFCdPFCm_2-rh',\n        '7Networks_RH_Default_PFCdPFCm_3-rh',\n        '7Networks_RH_Default_PFCv_1-rh',\n        '7Networks_RH_Default_PFCv_2-rh',\n        '7Networks_RH_Default_Temp_1-rh',\n        '7Networks_RH_Default_Temp_2-rh',\n        '7Networks_RH_Default_Temp_3-rh',\n        '7Networks_RH_DorsAttn_FEF_1-rh',\n        '7Networks_RH_DorsAttn_Post_1-rh',\n        '7Networks_RH_DorsAttn_Post_2-rh',\n        '7Networks_RH_DorsAttn_Post_3-rh',\n        '7Networks_RH_DorsAttn_Post_4-rh',\n        '7Networks_RH_DorsAttn_Post_5-rh',\n        '7Networks_RH_DorsAttn_PrCv_1-rh',\n        '7Networks_RH_Limbic_OFC_1-rh',\n        '7Networks_RH_Limbic_TempPole_1-rh',\n        '7Networks_RH_SalVentAttn_FrOperIns_1-rh',\n        '7Networks_RH_SalVentAttn_Med_1-rh',\n        '7Networks_RH_SalVentAttn_Med_2-rh',\n        '7Networks_RH_SalVentAttn_TempOccPar_1-rh',\n        '7Networks_RH_SalVentAttn_TempOccPar_2-rh',\n        '7Networks_RH_SomMot_1-rh',\n        '7Networks_RH_SomMot_2-rh',\n        '7Networks_RH_SomMot_3-rh',\n        '7Networks_RH_SomMot_4-rh',\n        '7Networks_RH_SomMot_5-rh',\n        '7Networks_RH_SomMot_6-rh',\n        '7Networks_RH_SomMot_7-rh',\n        '7Networks_RH_SomMot_8-rh',\n        '7Networks_RH_Vis_1-rh',\n        '7Networks_RH_Vis_2-rh',\n        '7Networks_RH_Vis_3-rh',\n        '7Networks_RH_Vis_4-rh',\n        '7Networks_RH_Vis_5-rh',\n        '7Networks_RH_Vis_6-rh',\n        '7Networks_RH_Vis_7-rh',\n        '7Networks_RH_Vis_8-rh'\n    ]\n    labels = np.array(labels)\n\n    # ---------------------------\n    # 3. Initialize Networks Dictionary\n    # ---------------------------\n    networks = {\n        'Visual': [],\n        'Somatomotor': [],\n        'DorsalAttention': [],\n        'VentralAttention': [],\n        'Limbic': [],\n        'Frontoparietal': [],\n        'Default': []\n    }\n\n    for i, label in enumerate(labels):\n        network_name = get_network_name(label)\n        if network_name:\n            region_name = get_region_name(label)\n            networks[network_name].append((i, region_name))\n\n    # ---------------------------\n    # 4. Load and Process Data\n    # ---------------------------\n    base_dir = '/home/cerna3/neuroconn/data/out/subjects/'\n    modes = ['EC', 'EO']\n\n    random.seed(42)\n    np.random.seed(42)\n\n    connectivity_data = load_connectivity_data(base_dir, participants, modes)\n    \n    print(\"\\nOrganizing connectivity data for NBS analysis...\")\n    organized_data = {group: {mode: {} for mode in modes} for group in set(participant_groups.values())}\n\n    total_participants = len(participants)\n    participants_processed = 0\n\n    for participant in participants:\n        group = participant_groups.get(participant)\n        if participant not in connectivity_data or not group:\n            continue\n        participant_data = connectivity_data[participant]\n        for mode in modes:\n            if mode not in participant_data:\n                continue\n            mode_data = participant_data[mode]\n            for state_window, matrix in mode_data.items():\n                state_window_str = f\"state_{state_window[0]}_window_{state_window[1]}\"\n                if state_window_str not in organized_data[group][mode]:\n                    organized_data[group][mode][state_window_str] = {}\n                vector_form = matrix[np.triu_indices_from(matrix, k=1)]\n                organized_data[group][mode][state_window_str][participant] = vector_form\n        participants_processed += 1\n        sys.stdout.write(f\"\\rOrganized data for participant {participant} ({participants_processed}/{total_participants})\")\n        sys.stdout.flush()\n\n    print(\"\\nFinished organizing connectivity data.\\n\")\n    \n    n_regions = labels.shape[0]\n    edge_indices = {}\n    idx = 0\n    edge_labels = []\n    for i in range(n_regions):\n        for j in range(i + 1, n_regions):\n            edge_indices[idx] = (i, j)\n            region_pair = (labels[i], labels[j])\n            edge_labels.append(region_pair)\n            idx += 1\n    \n    edge_indices_reverse = {}\n    idx = 0\n    for i in range(n_regions):\n        for j in range(i + 1, n_regions):\n            edge_indices_reverse[(i, j)] = idx\n            edge_indices_reverse[(j, i)] = idx\n            idx += 1\n\n    change_types = ['IMPC', 'DMNC', 'SNPC', 'DMPC', 'IMNC', 'SPNC']\n\n    total_steps = len(group_names) * len(change_types)\n    completed_steps = 0\n\n    aging_results = {}\n    taichi_results = {}\n    group1_means_all_global = {}\n    group2_means_all_global = {}\n    group3_means_all_global = {}\n    group3_means_all = {}\n    group1_means_all = {}\n    group2_means_all = {}\n\n    # ---------------------------\n    # 5. Run Analysis\n    # ---------------------------\n    for comparison_index, (comparison, (group1, group2)) in enumerate(group_names.items(), 1):\n        print(f\"\\nPerforming comparison {comparison_index}/{len(group_names)}: {group1} vs {group2}\")\n        logging.info(f\"\\nStarting comparison {comparison_index}/{len(group_names)}: {group1} vs {group2}\")\n\n        group1_data = organized_data[group1]\n        group2_data = organized_data[group2]\n\n        results_filename = f'results_{comparison}.pkl'\n        try:\n            with open(results_filename, 'rb') as f:\n                if comparison == 'oac_vs_tcoa':\n                    differences, nbs_results, group1_means_all, group2_means_all, convergence_stats, total_state_windows, group3_means_all, processed_windows = pickle.load(f)\n                else:\n                    differences, nbs_results, group1_means_all, group2_means_all, convergence_stats, total_state_windows, processed_windows = pickle.load(f)\n                print(f\"Loaded existing results for {comparison} from {results_filename}\")\n        except Exception as e:\n            print(f\"Could not load results from {results_filename}: {str(e)}\")\n            differences = {}\n            nbs_results = {}\n            group1_means_all = {}\n            group2_means_all = {}\n            convergence_stats = {}\n            total_state_windows = 0\n            group3_means_all = {}\n            processed_windows = set()\n\n        if comparison == 'oac_vs_tcoa':\n            group3_data = organized_data['YAC']\n            results = calculate_dynamic_group_differences_nbs(\n                group1_data,\n                group2_data,\n                comparison,\n                edge_labels,\n                alpha=0.05,\n                state_window_fraction=state_window_fraction,\n                max_permutations=max_permutations,\n                group3_data=group3_data,\n                results_filename=results_filename,\n                existing_differences=differences,\n                existing_nbs_results=nbs_results,\n                existing_group1_means_all=group1_means_all,\n                existing_group2_means_all=group2_means_all,\n                existing_convergence_stats=convergence_stats,\n                existing_total_state_windows=total_state_windows,\n                existing_group3_means_all=group3_means_all,\n                processed_windows=processed_windows\n            )\n        else:\n            results = calculate_dynamic_group_differences_nbs(\n                group1_data,\n                group2_data,\n                comparison,\n                edge_labels,\n                alpha=0.05,\n                state_window_fraction=state_window_fraction,\n                max_permutations=max_permutations,\n                results_filename=results_filename,\n                existing_differences=differences,\n                existing_nbs_results=nbs_results,\n                existing_group1_means_all=group1_means_all,\n                existing_group2_means_all=group2_means_all,\n                existing_convergence_stats=convergence_stats,\n                existing_total_state_windows=total_state_windows,\n                processed_windows=processed_windows\n            )\n\n        try:\n            with open(results_filename, 'wb') as f:\n                if comparison == 'oac_vs_tcoa':\n                    pickle.dump((results[0], results[1], results[2], results[3], results[4], results[5], results[6], processed_windows), f)\n                else:\n                    pickle.dump((results[0], results[1], results[2], results[3], results[4], results[5], processed_windows), f)\n            print(f\"Saved results for {comparison} to {results_filename}\")\n        except Exception as e:\n            print(f\"Error saving results to {results_filename}: {str(e)}\")\n            backup_filename = f'results_{comparison}_backup.pkl'\n            try:\n                with open(backup_filename, 'wb') as f:\n                    if comparison == 'oac_vs_tcoa':\n                        pickle.dump((results[0], results[1], results[2], results[3], results[4], results[5], results[6], processed_windows), f)\n                    else:\n                        pickle.dump((results[0], results[1], results[2], results[3], results[4], results[5], processed_windows), f)\n                print(f\"Results saved to backup file: {backup_filename}\")\n            except Exception as e:\n                print(f\"Error saving backup: {str(e)}\")\n\n        if comparison == 'oac_vs_tcoa':\n            differences, nbs_results, group1_means_all, group2_means_all, convergence_stats, total_state_windows, group3_means_all = results\n            taichi_results = {k: v for k, v in nbs_results.items()}\n            group1_means_all_global.update(group1_means_all)\n            group2_means_all_global.update(group2_means_all)\n            group3_means_all_global.update(group3_means_all)\n        else:\n            differences, nbs_results, group1_means_all, group2_means_all, convergence_stats, total_state_windows = results\n            group3_means_all = None\n            if comparison == 'oac_vs_yac':\n                aging_results = {k: v for k, v in nbs_results.items()}\n                group1_means_all_global.update(group1_means_all)\n                group2_means_all_global.update(group2_means_all)\n\n        for change_type_index, change_type in enumerate(change_types, 1):\n            df_filename = f'df_summary_{comparison}_{change_type}.pkl'\n            try:\n                with open(df_filename, 'rb') as f:\n                    existing_all_dfs, existing_summary_data, processed_keys = pickle.load(f)\n                    print(f\"Loaded existing summary data for {comparison}, {change_type} from {df_filename}\")\n            except Exception as e:\n                print(f\"Could not load summary from {df_filename}: {str(e)}\")\n                existing_all_dfs = []\n                existing_summary_data = {}\n                processed_keys = set()\n\n            print(\"Computing summary...\")\n\n            df_summary, df_summary_sheet = quantification_summary_to_excel_nbs(\n                differences,\n                nbs_results,\n                group1_means_all,\n                group2_means_all,\n                edge_labels,\n                networks,\n                labels,\n                change_type,\n                comparison,\n                writer=None,\n                group3_means_all=group3_means_all,\n                existing_all_dfs=existing_all_dfs,\n                existing_summary_data=existing_summary_data,\n                processed_keys=processed_keys,\n                summary_filename=df_filename,\n                edge_indices_reverse=edge_indices_reverse\n            )\n\n            try:\n                with open(df_filename, 'wb') as f:\n                    pickle.dump((df_summary, df_summary_sheet, processed_keys), f)\n                print(f\"Saved summary DataFrames for {comparison}, {change_type} to {df_filename}\")\n            except Exception as e:\n                print(f\"Error saving summary to {df_filename}: {str(e)}\")\n                backup_filename = f'df_summary_{comparison}_{change_type}_backup.pkl'\n                try:\n                    with open(backup_filename, 'wb') as f:\n                        pickle.dump((df_summary, df_summary_sheet, processed_keys), f)\n                    print(f\"Summary saved to backup file: {backup_filename}\")\n                except Exception as e:\n                    print(f\"Error saving backup: {str(e)}\")\n\n            completed_steps += 1\n            progress = (completed_steps / total_steps) * 100\n            sys.stdout.write(f\"\\rOverall Progress: {completed_steps}/{total_steps} ({progress:.2f}%)\")\n            sys.stdout.flush()\n\n    print(\"\\nAll comparisons and change types processed.\")\n\n    with pd.ExcelWriter('quantification_summary_dynamic_nbs.xlsx', engine='xlsxwriter') as writer:\n        for comparison_index, (comparison, (group1, group2)) in enumerate(group_names.items(), 1):\n            for change_type_index, change_type in enumerate(change_types, 1):\n                df_filename = f'df_summary_{comparison}_{change_type}.pkl'\n                if os.path.exists(df_filename):\n                    with open(df_filename, 'rb') as f:\n                        df_summary, df_summary_sheet, _ = pickle.load(f)\n                    sheet_name = f'{comparison}_{change_type.lower()}'\n                    df_summary.to_excel(writer, sheet_name=sheet_name, index=False)\n                    sheet_name_summary = f'{comparison}_{change_type.lower()}_summary'\n                    df_summary_sheet.to_excel(writer, sheet_name=sheet_name_summary, index=False)\n                    print(f\"Sheets '{sheet_name}' and '{sheet_name_summary}' written to Excel.\")\n    print(\"\\nAnalysis complete. Results saved to 'quantification_summary_dynamic_nbs.xlsx'\")\n        \n    total_time_taken = (time.time() - start_time) / 60\n    print(f\"Total processing time: {total_time_taken:.2f} minutes.\")\n\n    if 'aging_results' in locals() and 'taichi_results' in locals() and group3_means_all_global:\n        yac_means = group3_means_all_global\n        compensatory_results = identify_compensatory_connections(\n            aging_results=aging_results,\n            taichi_results=taichi_results,\n            yac_means=yac_means,\n            comparison='oac_vs_tcoa',\n            labels=labels,\n            edge_indices_reverse=edge_indices_reverse,\n            group1_means_all=group1_means_all_global,\n            group2_means_all=group2_means_all_global\n        )\n        \n        compensatory_filename = 'compensatory_results.pkl'\n        with open(compensatory_filename, 'wb') as f:\n            pickle.dump(compensatory_results, f)\n        print(f\"Compensatory results saved to {compensatory_filename}\")\n    else:\n        print(\"Not enough data to perform compensatory analysis.\")\n\nif __name__ == '__main__':\n    main()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}